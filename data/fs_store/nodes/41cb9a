{"node_id": "41cb9a", "title": "Prompt caching (Anthropic)", "path": "harness > Agent harness capabilities > Prompt caching (Anthropic)", "content": "The harness enables Anthropic's prompt caching feature to reduce redundant token processing.\n\n**How it works:**\n\n* Caches portions of the prompt that repeat across turns\n* Significantly reduces latency and cost for long system prompts\n* Automatically skips for non-Anthropic models\n\n**Why it's useful:**\n\n* System prompts (especially with filesystem docs) can be 5k+ tokens\n* These repeat every turn without caching\n* Caching provides \\~10x speedup and cost reduction\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/deepagents/harness.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\n> To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt", "summary": "Enables Anthropic's prompt caching to reduce redundant token processing, enhancing speed and reducing costs.", "keywords": ["Prompt caching", "Anthropic", "Token processing", "Latency reduction", "Cost reduction", "System prompts"]}