{"node_id": "b84b2b", "title": "Tailor configurations by risk", "path": "human-in-the-loop > Human-in-the-loop > Best practices > Tailor configurations by risk", "content": "Configure different tools based on their risk level:\n\n```python  theme={null}\ninterrupt_on = {\n    # High risk: full control (approve, edit, reject)\n    \"delete_file\": {\"allowed_decisions\": [\"approve\", \"edit\", \"reject\"]},\n    \"send_email\": {\"allowed_decisions\": [\"approve\", \"edit\", \"reject\"]},\n\n    # Medium risk: no editing allowed\n    \"write_file\": {\"allowed_decisions\": [\"approve\", \"reject\"]},\n\n    # Low risk: no interrupts\n    \"read_file\": False,\n    \"list_files\": False,\n}\n```\n\n***\n\n<Callout icon=\"pen-to-square\" iconType=\"regular\">\n  [Edit this page on GitHub](https://github.com/langchain-ai/docs/edit/main/src/oss/deepagents/human-in-the-loop.mdx) or [file an issue](https://github.com/langchain-ai/docs/issues/new/choose).\n</Callout>\n\n<Tip icon=\"terminal\" iconType=\"regular\">\n  [Connect these docs](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.\n</Tip>\n\n\n---\n\n> To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt", "summary": "Configure tools with different levels of human intervention based on their risk level.", "keywords": ["risk level", "human intervention", "tool configuration", "approve", "edit", "reject", "high risk", "medium risk", "low risk"]}